SWIPE NEXT SWIPE NEXT 1.LINEAR REGRESSION 1.LINEAR REGRESSION 2.LOGISTIC REGRESSION 2.LOGISTIC REGRESSION 3.POLYNOMIAL REGRESSION 3.POLYNOMIAL REGRESSIONUsed for predicting a continuous output variable Used for predicting a continuous output variable based on one or more input variables based on one or more input variables Used for binary classification problems Used for binary classification problems Used for modeling relationships between input Used for modeling relationships between input and output variables that are not linear and output variables that are not linear SWIPE NEXT SWIPE NEXT Akash Raj Data Scientist 4.RIDGE REGRESSION 4.RIDGE REGRESSION 5.LASSO REGRESSION 5.LASSO REGRESSION 6.ELASTICNET REGRESSION 6.
ELASTICNET REGRESSIONUsed to prevent overfitting in linear regression Used to prevent overfitting in linear regression by adding a penalty term to the loss function by adding a penalty term to the loss function Used to perform variable selection in linear Used to perform variable selection in linear regression by shrinking coefficients towards zero regression by shrinking coefficients towards zero Combines the Ridge and Lasso regularization Combines the Ridge and Lasso regularization techniques to handle both multicollinearity and techniques to handle both multicollinearity and variable selection variable selection SWIPE NEXT SWIPE NEXT Akash Raj Data Scientist 7. DECISION TREES 7.DECISION TREES 8.RANDOM FORESTS 8.RANDOM FORESTS 9.GRADIENT BOOSTING MACHINES 9.
GRADIENT BOOSTING MACHINESUsed for both classification and regression problems byUsed for both classification and regression problems by recursively partitioning the input space into smallerrecursively partitioning the input space into smaller regionsregions An ensemble of decision trees that reduces variance andAn ensemble of decision trees that reduces variance and improves generalization by averaging the results ofimproves generalization by averaging the results of multiple treesmultiple trees An ensemble of decision trees that improves performanceAn ensemble of decision trees that improves performance by fitting each subsequent tree to the residual errors ofby fitting each subsequent tree to the residual errors of the previous treesthe previous trees SWIPE NEXT SWIPE NEXT Akash Raj Data Scientist 10. EXTREME GRADIENT BOOSTING (XGBOOST) 10.EXTREME GRADIENT BOOSTING (XGBOOST) 11.LIGHT GRADIENT BOOSTING MACHINE (LIGHTGBM) 11.LIGHT GRADIENT BOOSTING MACHINE (LIGHTGBM) 12.CATBOOST 12.
CATBOOSTA highly optimized implementation of gradient A highly optimized implementation of gradient boosting that includes many advanced features boosting that includes many advanced features Another optimized implementation of gradient boostingAnother optimized implementation of gradient boosting that can handle large datasets and high-dimensionalthat can handle large datasets and high-dimensional featuresfeatures A gradient boosting algorithm that can handle A gradient boosting algorithm that can handle categorical features and missing values categorical features and missing values SWIPE NEXT SWIPE NEXT Akash Raj Data Scientist 13. K-NEAREST NEIGHBORS (KNN) 13.K-NEAREST NEIGHBORS (KNN) 14.SUPPORT VECTOR MACHINES (SVM) 14.SUPPORT VECTOR MACHINES (SVM) 15.NAIVE BAYES 15.
NAIVE BAYESA non-parametric algorithm that predicts the output of a A non-parametric algorithm that predicts the output of a new example by finding the K closest examples in the new example by finding the K closest examples in the training data and taking their average training data and taking their average A parametric algorithm that finds the hyperplane that A parametric algorithm that finds the hyperplane that separates two classes with the maximum margin separates two classes with the maximum margin A probabilistic algorithm that uses Bayes' theorem toA probabilistic algorithm that uses Bayes' theorem to calculate the probability of a new example belonging tocalculate the probability of a new example belonging to each classeach class SWIPE NEXT SWIPE NEXT Akash Raj Data Scientist SWIPE NEXT SWIPE NEXT16. ARTIFICIAL NEURAL NETWORKS 16.ARTIFICIAL NEURAL NETWORKS 17.CONVOLUTIONAL NEURAL NETWORKS (CNN) 17.CONVOLUTIONAL NEURAL NETWORKS (CNN) 18.RECURRENT NEURAL NETWORKS (RNN) 18.
RECURRENT NEURAL NETWORKS (RNN)A set of algorithms that attempt to mimic the A set of algorithms that attempt to mimic the structure and function of the human brain structure and function of the human brain A type of neural network that is particularly good at imageA type of neural network that is particularly good at image recognition and other tasks that involve spatial datarecognition and other tasks that involve spatial data A type of neural network that is designed for processingA type of neural network that is designed for processing sequential data, such as time series or natural languagesequential data, such as time series or natural language Akash Raj Data Scientist 19. LONG SHORT-TERM MEMORY (LSTM) 19.LONG SHORT-TERM MEMORY (LSTM) 20.AUTOENCODERS 20.AUTOENCODERS 21.GENERATIVE ADVERSARIAL NETWORKS (GAN) 21.
GENERATIVE ADVERSARIAL NETWORKS (GAN)A type of RNN that can remember information over longerA type of RNN that can remember information over longer periods of time and avoid the vanishing gradient problemperiods of time and avoid the vanishing gradient problem A type of neural network that is used for dimensionalityA type of neural network that is used for dimensionality reduction, data compression, and unsupervised featurereduction, data compression, and unsupervised feature learninglearning A type of neural network that can generate new examples that areA type of neural network that can generate new examples that are similar to the training data by learning to generate realisticsimilar to the training data by learning to generate realistic examples while being trained to distinguish between real and fakeexamples while being trained to distinguish between real and fake examplesexamples SWIPE NEXT SWIPE NEXT Akash Raj Data Scientist 22. PRINCIPAL COMPONENT ANALYSIS (PCA) 22.
PRINCIPAL COMPONENT ANALYSIS (PCA) 23. T-SNE 23.T-SNE 24.K-MEANS CLUSTERING 24.K-MEANS CLUSTERINGA technique for reducing the dimensionality of high- A technique for reducing the dimensionality of high- dimensional datasets by projecting them onto a lower- dimensional datasets by projecting them onto a lower- dimensional space that captures most of the variation dimensional space that captures most of the variation t-Distributed Stochastic Neighbor Embedding (t-SNE) is at-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique for visualizing high-dimensional datasets in two ortechnique for visualizing high-dimensional datasets in two or three dimensions by mapping similar examples to nearby pointsthree dimensions by mapping similar examples to nearby points and dissimilar examples to distant points.and dissimilar examples to distant points.
A unsupervised learning algorithm that partitions a dataset A unsupervised learning algorithm that partitions a dataset into K clusters by minimizing the distance between each into K clusters by minimizing the distance between each example and the centroid of its cluster example and the centroid of its cluster SWIPE NEXT SWIPE NEXT Akash Raj Data Scientist 25. HIERARCHICAL CLUSTERING 25.HIERARCHICAL CLUSTERING 26.GAUSSIAN MIXTURE MODELS (GMM) 26.GAUSSIAN MIXTURE MODELS (GMM) 27.HIDDEN MARKOV MODELS (HMM) 27.
HIDDEN MARKOV MODELS (HMM)A unsupervised learning algorithm that builds a A unsupervised learning algorithm that builds a hierarchy of clusters by recursively merging the hierarchy of clusters by recursively merging the closest pairs of clusters closest pairs of clusters A probabilistic clustering algorithm that models each A probabilistic clustering algorithm that models each cluster as a Gaussian distribution and estimates the cluster as a Gaussian distribution and estimates the parameters using the expectation-maximization algorithm parameters using the expectation-maximization algorithm A type of generative model that is used for sequence A type of generative model that is used for sequence modeling, such as speech recognition, natural language modeling, such as speech recognition, natural language processing, and bioinformatics processing, and bioinformatics SWIPE NEXT SWIPE NEXT Akash Raj Data Scientist 28. LINEAR DISCRIMINANT ANALYSIS (LDA) 28.
LINEAR DISCRIMINANT ANALYSIS (LDA) 29. QUADRATIC DISCRIMINANT ANALYSIS (QDA) 29.QUADRATIC DISCRIMINANT ANALYSIS (QDA) 30.ENSEMBLE LEARNING 30.ENSEMBLE LEARNINGA supervised learning algorithm that finds a linear A supervised learning algorithm that finds a linear combination of features that maximizes the combination of features that maximizes the separation between two or more classes separation between two or more classes Similar to LDA, but allows for quadratic decisionSimilar to LDA, but allows for quadratic decision boundariesboundaries A technique for combining multiple models to improve A technique for combining multiple models to improve performance, reduce variance, and increase robustness.performance, reduce variance, and increase robustness.
Examples include bagging, boosting, and stacking Examples include bagging, boosting, and stacking SWIPE NEXT SWIPE NEXT Akash Raj Data Scientist LET'S TAKE YOUR DATA SCIENCE SKILLS TO THE NEXT LEVELFROM BEGINNER TO PRO FOLLOW ME TODAY Akash Raj Data Scientist
